from fastapi import FastAPI, APIRouter, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import redis
import uuid
import json
import requests
import logging
import asyncio
from datetime import datetime
from collections import defaultdict
import time
import re

##################### LOGGING ####################

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

##################### CONFIGURATION ####################

REDIS_HOST = "localhost"
REDIS_PORT = 6379
REDIS_PASSWORD = None
REDIS_DB = 0

QWEN_HOST = "localhost"
QWEN_PORT = 8001
QWEN_MODEL = "qwen-7b-instruct"
QWEN_TIMEOUT = 60

# Redis prefixes
SESSION_PREFIX = "session:"
SUMMARY_PREFIX = "summary:"
USER_PROFILE_PREFIX = "user_profile:"
DIALOG_STATE_PREFIX = "dialog_state:"  # üÜï INTENT STORAGE
SESSION_TTL = 86400 * 7  # 7 days

# Memory limits
MAX_HISTORY_LENGTH = 30
CONTEXT_WINDOW = 6
MAX_RESPONSE_TOKENS = 300
MAX_SUMMARY_TOKENS = 150
SUMMARY_THRESHOLD = 8
PROFILE_UPDATE_THRESHOLD = 5

# Concurrency
LLM_CONCURRENCY = 2
RATE_LIMIT_REQUESTS = 1
RATE_LIMIT_WINDOW = 3

# üÜï USER LEVELS
USER_LEVELS = ["beginner", "junior", "middle", "senior", "expert"]
RESPONSE_MODES = ["learn", "debug", "inspect", "design", "quick"]

##################### REDIS CONNECTION ####################

r = redis.Redis(
    host=REDIS_HOST,
    port=REDIS_PORT,
    password=REDIS_PASSWORD,
    db=REDIS_DB,
    decode_responses=True,
    socket_connect_timeout=5,
    socket_keepalive=True
)

##################### CONCURRENCY CONTROL ####################

llm_semaphore = asyncio.Semaphore(LLM_CONCURRENCY)
rate_limit_tracker: Dict[str, List[float]] = defaultdict(list)

def check_rate_limit(user_id: str, session_id: str) -> bool:
    key = f"{user_id}:{session_id}"
    now = time.time()
    
    rate_limit_tracker[key] = [
        req_time for req_time in rate_limit_tracker[key]
        if now - req_time < RATE_LIMIT_WINDOW
    ]
    
    if len(rate_limit_tracker[key]) >= RATE_LIMIT_REQUESTS:
        return False
    
    rate_limit_tracker[key].append(now)
    return True

##################### PYDANTIC MODELS ####################

class ChatRequest(BaseModel):
    user_id: str
    session_id: str
    message: str

class ChatResponse(BaseModel):
    user_id: str
    session_id: str
    response: str
    timestamp: str
    intent: Optional[Dict[str, Any]] = None
    summary: Optional[str] = None
    profile_updated: Optional[bool] = None
    tokens_used: Optional[int] = None
    rules_applied: Optional[List[str]] = None  # üÜï

class SessionCreate(BaseModel):
    user_id: str
    metadata: Optional[Dict[str, Any]] = None

class ProfileUpdate(BaseModel):
    user_id: str
    profile_data: Dict[str, Any]

##################### UTILITIES ####################

def generate_id() -> str:
    return str(uuid.uuid4())

##################### üÜï STEP 2: USER PROFILE (PASSPORT) ####################

def get_user_profile(user_id: str) -> Dict[str, Any]:
    """
    üìã USER PROFILE = –ü–ê–°–ü–û–†–¢ –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø
    
    –•—Ä–∞–Ω–∏—Ç –¢–û–õ–¨–ö–û —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç—ã:
    - –∏–º—è
    - –≤–æ–∑—Ä–∞—Å—Ç
    - —Ä–æ–ª—å (student/engineer/designer)
    - —É—Ä–æ–≤–µ–Ω—å (beginner/junior/middle/senior/expert)
    - —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
    - —è–∑—ã–∫ –æ–±—â–µ–Ω–∏—è
    
    –ù–ï –•–†–ê–ù–ò–¢:
    - —Ç–µ–∫—É—â—É—é –∑–∞–¥–∞—á—É
    - —ç–º–æ—Ü–∏–∏
    - –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤–æ–ø—Ä–æ—Å—ã
    """
    key = f"{USER_PROFILE_PREFIX}{user_id}"
    data = r.get(key)
    
    if not data:
        return {
            "name": None,
            "age": None,
            "role": None,  # student/engineer/designer/etc
            "level": "junior",  # beginner/junior/middle/senior/expert
            "tech_stack": [],
            "language": "en",  # en/ru/etc
            "interests": [],
            "learning_goals": []
        }
    
    return json.loads(data)

def update_user_profile(user_id: str, profile_data: Dict[str, Any]) -> None:
    key = f"{USER_PROFILE_PREFIX}{user_id}"
    r.set(key, json.dumps(profile_data), ex=SESSION_TTL * 4)
    logger.info(f"‚úì Profile updated: {user_id}")

def merge_profile_facts(old: Dict[str, Any], new: Dict[str, Any]) -> Dict[str, Any]:
    merged = old.copy()
    
    for key, value in new.items():
        if key not in merged:
            merged[key] = value
        elif value is None:
            continue
        elif isinstance(value, list):
            if not isinstance(merged[key], list):
                merged[key] = []
            merged[key] = list(set(merged[key] + value))
        elif isinstance(value, dict):
            if not isinstance(merged[key], dict):
                merged[key] = {}
            merged[key].update(value)
        else:
            if value:
                merged[key] = value
    
    return merged

##################### üÜï STEP 3: DIALOG STATE (INTENT) ####################

def get_dialog_state(user_id: str, session_id: str) -> Dict[str, Any]:
    """
    üéØ DIALOG STATE = –¢–ï–ö–£–©–ê–Ø –¶–ï–õ–¨ –ò –ö–û–ù–¢–ï–ö–°–¢
    
    –û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã:
    - –ß—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç –°–ï–ô–ß–ê–°?
    - –í –∫–∞–∫–æ–º —Ä–µ–∂–∏–º–µ —Ä–∞–±–æ—Ç–∞–µ–º? (learn/debug/inspect/design)
    - –ö–∞–∫–æ–π —É—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω—É–∂–µ–Ω?
    - –ß—Ç–æ –£–ñ–ï –ø–æ–Ω—è—Ç–Ω–æ?
    - –ß—Ç–æ –ó–ê–ü–†–ï–©–ï–ù–û –æ–±—ä—è—Å–Ω—è—Ç—å?
    
    –ë–ï–ó –≠–¢–û–ì–û LLM –í–°–ï–ì–î–ê –ë–£–î–ï–¢ –¢–£–ü–ò–¢–¨
    """
    key = f"{DIALOG_STATE_PREFIX}{user_id}:{session_id}"
    data = r.get(key)
    
    if not data:
        return {
            "current_goal": None,  # "learn React", "debug error", "design system"
            "mode": "learn",  # learn/debug/inspect/design/quick
            "detail_level": "normal",  # brief/normal/detailed
            "understood_concepts": [],  # —á—Ç–æ —É–∂–µ –ø–æ–Ω—è—Ç–Ω–æ
            "forbidden_topics": [],  # —á—Ç–æ –ù–ï –æ–±—ä—è—Å–Ω—è—Ç—å
            "context_type": None,  # code/theory/architecture/practice
            "last_updated": None
        }
    
    return json.loads(data)

def update_dialog_state(user_id: str, session_id: str, state: Dict[str, Any]) -> None:
    key = f"{DIALOG_STATE_PREFIX}{user_id}:{session_id}"
    state["last_updated"] = datetime.utcnow().isoformat()
    r.set(key, json.dumps(state), ex=SESSION_TTL)
    logger.info(f"‚úì Dialog state updated: {state.get('current_goal', 'unknown')}")

##################### SESSION FUNCTIONS ####################

def create_session(user_id: str, metadata: Optional[Dict] = None) -> str:
    session_id = generate_id()
    key = f"{SESSION_PREFIX}{user_id}:{session_id}"
    
    r.hset(key, mapping={
        "messages": json.dumps([]),
        "created_at": datetime.utcnow().isoformat(),
        "updated_at": datetime.utcnow().isoformat(),
        "metadata": json.dumps(metadata or {}),
        "message_count": "0",
        "last_profile_check": "0"
    })
    r.expire(key, SESSION_TTL)
    
    logger.info(f"‚úì Session created: {user_id}:{session_id}")
    return session_id

def get_session(user_id: str, session_id: str) -> Optional[Dict]:
    key = f"{SESSION_PREFIX}{user_id}:{session_id}"
    data = r.hgetall(key)
    
    if not data:
        return None
    
    return {
        "user_id": user_id,
        "session_id": session_id,
        "messages": json.loads(data.get("messages", "[]")),
        "created_at": data.get("created_at", ""),
        "updated_at": data.get("updated_at", ""),
        "metadata": json.loads(data.get("metadata", "{}")),
        "message_count": int(data.get("message_count", 0)),
        "last_profile_check": int(data.get("last_profile_check", 0))
    }

def update_session(user_id: str, session_id: str, messages: List[Dict], last_profile_check: Optional[int] = None) -> bool:
    key = f"{SESSION_PREFIX}{user_id}:{session_id}"
    
    if not r.exists(key):
        return False
    
    if len(messages) > MAX_HISTORY_LENGTH:
        messages = messages[-MAX_HISTORY_LENGTH:]
    
    update_data = {
        "messages": json.dumps(messages),
        "updated_at": datetime.utcnow().isoformat(),
        "message_count": str(len(messages))
    }
    
    if last_profile_check is not None:
        update_data["last_profile_check"] = str(last_profile_check)
    
    r.hset(key, mapping=update_data)
    r.expire(key, SESSION_TTL)
    
    return True

def delete_session(user_id: str, session_id: str) -> bool:
    session_key = f"{SESSION_PREFIX}{user_id}:{session_id}"
    summary_key = f"{SUMMARY_PREFIX}{user_id}:{session_id}"
    state_key = f"{DIALOG_STATE_PREFIX}{user_id}:{session_id}"
    
    deleted = r.delete(session_key, summary_key, state_key)
    
    rate_key = f"{user_id}:{session_id}"
    if rate_key in rate_limit_tracker:
        del rate_limit_tracker[rate_key]
    
    logger.info(f"‚úì Session deleted: {user_id}:{session_id}")
    return deleted > 0

##################### üÜï STEP 4: SUMMARY (–¢–û–ß–ö–ê –ù–ê –ö–ê–†–¢–ï) ####################

def get_summary(user_id: str, session_id: str) -> str:
    key = f"{SUMMARY_PREFIX}{user_id}:{session_id}"
    data = r.get(key)
    return data if data else ""

def update_summary(user_id: str, session_id: str, summary: str) -> None:
    key = f"{SUMMARY_PREFIX}{user_id}:{session_id}"
    r.set(key, summary, ex=SESSION_TTL)
    logger.info(f"‚úì Summary updated: {user_id}:{session_id}")

async def generate_summary(old_summary: str, messages: List[Dict]) -> str:
    """
    üìç SUMMARY = –¢–û–ß–ö–ê –ù–ê –ö–ê–†–¢–ï
    
    –û—Ç–≤–µ—á–∞–µ—Ç –¢–û–õ–¨–ö–û –Ω–∞:
    - –û —á—ë–º –¥–∏–∞–ª–æ–≥ –°–ï–ô–ß–ê–°?
    - –ö –∫–∞–∫–æ–º—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –∏–¥—ë–º?
    - –ß—Ç–æ —É–∂–µ —Ä–µ—à–µ–Ω–æ?
    
    –ù–ï –î–û–õ–ñ–ï–ù:
    - –ü–µ—Ä–µ—Å–∫–∞–∑—ã–≤–∞—Ç—å –≤—Å—é –∏—Å—Ç–æ—Ä–∏—é
    - –°–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Ç–∞—Ä—ã–µ —Ç–µ–º—ã
    """
    
    recent = "\n".join([
        f"{m['role'].capitalize()}: {m['content'][:150]}"
        for m in messages[-4:]
    ])
    
    prompt = f"""Analyze current dialog state.

Previous state: {old_summary if old_summary else "New conversation"}

Recent exchange:
{recent}

Create concise summary (max 80 words) answering ONLY:
1. What is the CURRENT topic/goal?
2. What result are we working toward?
3. What has been DECIDED/SOLVED?

FORBIDDEN:
- Full history retelling
- Old/resolved topics
- User personal details

Focus on CURRENT STATE, not past."""

    llm_messages = [{"role": "user", "content": prompt}]
    
    try:
        result = await send_to_llm(llm_messages, temperature=0.3, max_tokens=MAX_SUMMARY_TOKENS)
        return result["content"].strip()
    except Exception as e:
        logger.error(f"‚úó Summary generation failed: {e}")
        return old_summary

##################### üÜï STEP 3: INTENT EXTRACTION ####################

async def extract_intent(user_message: str, current_state: Dict[str, Any], profile: Dict[str, Any]) -> Dict[str, Any]:
    """
    üéØ –ò–ó–í–õ–ï–ß–ï–ù–ò–ï DIALOG STATE –ò–ó –°–û–û–ë–©–ï–ù–ò–Ø
    
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç:
    - current_goal: —á—Ç–æ —Ö–æ—á–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
    - mode: learn/debug/inspect/design/quick
    - detail_level: brief/normal/detailed
    - understood_concepts: —á—Ç–æ –£–ñ–ï –ø–æ–Ω—è—Ç–Ω–æ
    - forbidden_topics: —á—Ç–æ –ù–ï –æ–±—ä—è—Å–Ω—è—Ç—å
    """
    
    current_goal = current_state.get("current_goal", "unknown")
    understood = ", ".join(current_state.get("understood_concepts", [])[:5])
    
    prompt = f"""Extract dialog intent from user message.

USER LEVEL: {profile.get('level', 'junior')}
CURRENT GOAL: {current_goal}
UNDERSTOOD: {understood}

USER MESSAGE: "{user_message}"

Determine and return JSON:
{{
  "current_goal": "brief description of what user wants NOW",
  "mode": "learn|debug|inspect|design|quick",
  "detail_level": "brief|normal|detailed",
  "understood_concepts": ["concept1", "concept2"],
  "forbidden_topics": ["basics", "already explained"],
  "context_type": "code|theory|architecture|practice"
}}

RULES:
- If user says "I know X" ‚Üí add X to understood_concepts, forbidden_topics
- If user asks "how to debug" ‚Üí mode: "debug"
- If user asks "explain" ‚Üí mode: "learn"
- If user asks "show code" ‚Üí mode: "inspect"
- If user says "briefly" ‚Üí detail_level: "brief"
- Return ONLY valid JSON, no text."""

    llm_messages = [{"role": "user", "content": prompt}]
    
    try:
        result = await send_to_llm(llm_messages, temperature=0.1, max_tokens=200)
        response = result["content"].strip()
        
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            intent = json.loads(json_match.group())
            logger.info(f"‚úì Intent extracted: {intent.get('mode')} - {intent.get('current_goal')}")
            return intent
        
        logger.warning("‚úó No JSON in intent extraction")
        return current_state
        
    except Exception as e:
        logger.error(f"‚úó Intent extraction failed: {e}")
        return current_state

##################### üÜï STEP 6: CONTROLLER (–ü–†–ê–í–ò–õ–ê) ####################

def build_response_rules(profile: Dict[str, Any], state: Dict[str, Any]) -> List[str]:
    """
    üö¶ CONTROLLER - –ü–†–ê–í–ò–õ–ê –û–¢–í–ï–¢–ê
    
    –†–µ—à–∞–µ—Ç –ø–µ—Ä–µ–¥ –ö–ê–ñ–î–´–ú –æ—Ç–≤–µ—Ç–æ–º:
    - –ú–æ–∂–Ω–æ –ª–∏ –¥–∞–≤–∞—Ç—å –∫–æ–¥?
    - –ú–æ–∂–Ω–æ –ª–∏ –ø–æ–≤—Ç–æ—Ä—è—Ç—å?
    - –ú–æ–∂–Ω–æ –ª–∏ –æ–±—ä—è—Å–Ω—è—Ç—å –±–∞–∑—É?
    - –ú–æ–∂–Ω–æ –ª–∏ —É—Ö–æ–¥–∏—Ç—å –≤ —Å—Ç–æ—Ä–æ–Ω—É?
    
    LLM = –ò–°–ü–û–õ–ù–ò–¢–ï–õ–¨
    BACKEND = –ú–û–ó–ì
    """
    
    rules = []
    
    level = profile.get("level", "junior")
    mode = state.get("mode", "learn")
    detail_level = state.get("detail_level", "normal")
    understood = state.get("understood_concepts", [])
    forbidden = state.get("forbidden_topics", [])
    
    # 1. –ü—Ä–∞–≤–∏–ª–∞ –ø–æ —É—Ä–æ–≤–Ω—é
    if level == "beginner":
        rules.append("Explain like to a beginner, use simple terms")
        rules.append("Include basic examples")
        rules.append("NO assumptions about prior knowledge")
    elif level in ["middle", "senior", "expert"]:
        rules.append("Skip basic explanations")
        rules.append("Use technical terms freely")
        rules.append("Focus on advanced concepts")
    
    # 2. –ü—Ä–∞–≤–∏–ª–∞ –ø–æ —Ä–µ–∂–∏–º—É
    if mode == "debug":
        rules.append("Focus on finding the error")
        rules.append("Provide specific solution, not theory")
        rules.append("Show corrected code")
    elif mode == "quick":
        rules.append("Answer in 1-2 sentences maximum")
        rules.append("NO long explanations")
    elif mode == "design":
        rules.append("Focus on architecture and patterns")
        rules.append("Explain trade-offs")
    
    # 3. –ü—Ä–∞–≤–∏–ª–∞ –ø–æ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
    if detail_level == "brief":
        rules.append("Keep response under 100 words")
        rules.append("Only essential information")
    elif detail_level == "detailed":
        rules.append("Provide thorough explanation")
        rules.append("Include examples and edge cases")
    
    # 4. –ó–∞–ø—Ä–µ—Ç—ã
    if understood:
        rules.append(f"DO NOT explain these (user knows): {', '.join(understood[:3])}")
    
    if forbidden:
        rules.append(f"FORBIDDEN topics: {', '.join(forbidden[:3])}")
    
    # 5. –û–±—â–∏–µ –ø—Ä–∞–≤–∏–ª–∞
    rules.append("NO repetition of previous answers")
    rules.append("Stay on topic, no tangents")
    rules.append("If asked something off-topic, politely redirect")
    
    return rules

##################### üÜï STEP 7: SMART PROMPT ASSEMBLY ####################

def build_system_prompt(profile: Dict[str, Any], state: Dict[str, Any], summary: str, messages: List[Dict]) -> str:
    """
    üß† –£–ú–ù–ê–Ø –°–ë–û–†–ö–ê –ü–†–û–ú–ü–¢–ê
    
    –°–æ–±–∏—Ä–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑:
    1. –ö—Ä–∞—Ç–∫–∏–π –ø—Ä–æ—Ñ–∏–ª—å (–ø–∞—Å–ø–æ—Ä—Ç)
    2. Dialog state (—Ç–µ–∫—É—â–∞—è —Ü–µ–ª—å)
    3. Summary (–≥–¥–µ –º—ã —Å–µ–π—á–∞—Å)
    4. –ñ—ë—Å—Ç–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –æ—Ç–≤–µ—Ç–∞
    5. –¢–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ CONTEXT_WINDOW —Å–æ–æ–±—â–µ–Ω–∏–π
    """
    
    parts = ["You are a helpful AI assistant."]
    
    # 1. USER PROFILE (PASSPORT)
    profile_lines = []
    if profile.get("name"):
        profile_lines.append(f"Name: {profile['name']}")
    if profile.get("role"):
        profile_lines.append(f"Role: {profile['role']}")
    profile_lines.append(f"Level: {profile.get('level', 'junior')}")
    if profile.get("tech_stack"):
        profile_lines.append(f"Tech: {', '.join(profile['tech_stack'][:3])}")
    
    if profile_lines:
        parts.append(f"\nUSER PROFILE:\n" + "\n".join(profile_lines))
    
    # 2. DIALOG STATE (INTENT)
    if state.get("current_goal"):
        parts.append(f"\nCURRENT GOAL: {state['current_goal']}")
        parts.append(f"MODE: {state.get('mode', 'learn')}")
        parts.append(f"DETAIL LEVEL: {state.get('detail_level', 'normal')}")
    
    if state.get("understood_concepts"):
        parts.append(f"USER ALREADY KNOWS: {', '.join(state['understood_concepts'][:5])}")
    
    # 3. SUMMARY (WHERE WE ARE)
    if summary:
        parts.append(f"\nCONVERSATION STATE:\n{summary[:250]}")
    
    # 4. RESPONSE RULES
    rules = build_response_rules(profile, state)
    if rules:
        parts.append("\nRESPONSE RULES:")
        for rule in rules[:8]:  # max 8 rules
            parts.append(f"- {rule}")
    
    # 5. RECENT MESSAGES (only last CONTEXT_WINDOW)
    if messages:
        recent = messages[-CONTEXT_WINDOW:]
        history = "\n".join([
            f"{m['role'].capitalize()}: {m['content'][:200]}"
            for m in recent
        ])
        parts.append(f"\nRECENT EXCHANGE:\n{history}")
    
    return "\n".join(parts)

##################### LLM CLIENT ####################

BASE_URL = f"http://{QWEN_HOST}:{QWEN_PORT}/v1/chat/completions"

async def send_to_llm(messages: List[Dict[str, str]], temperature: float = 0.7, max_tokens: int = MAX_RESPONSE_TOKENS) -> Dict[str, Any]:
    async with llm_semaphore:
        logger.info(f"üîÑ LLM request (queue: {LLM_CONCURRENCY - llm_semaphore._value}/{LLM_CONCURRENCY})")
        
        try:
            payload = {
                "messages": messages,
                "model": QWEN_MODEL,
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: requests.post(BASE_URL, json=payload, timeout=QWEN_TIMEOUT)
            )
            response.raise_for_status()
            
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            tokens_used = result.get("usage", {}).get("total_tokens", len(content.split()))
            
            logger.info(f"‚úì LLM response: {len(content)} chars, ~{tokens_used} tokens")
            
            return {
                "content": content,
                "tokens_used": tokens_used
            }
            
        except requests.exceptions.Timeout:
            logger.error("‚úó LLM timeout")
            raise HTTPException(status_code=504, detail="LLM timeout")
        except requests.exceptions.RequestException as e:
            logger.error(f"‚úó LLM error: {e}")
            raise HTTPException(status_code=502, detail=f"LLM error: {str(e)}")
        except Exception as e:
            logger.error(f"‚úó Unexpected error: {e}")
            raise HTTPException(status_code=500, detail=f"Internal error: {str(e)}")

##################### üÜï STEP 8: RESPONSE VALIDATION ####################

def validate_response(response: str, state: Dict[str, Any], previous_responses: List[str]) -> bool:
    """
    ‚úÖ –ü–†–û–í–ï–†–ö–ê –û–¢–í–ï–¢–ê
    
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
    - –†–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –ª–∏ –æ—Ç–≤–µ—Ç?
    - –ù–µ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç –ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ?
    - –ù–µ —É—à—ë–ª –ª–∏ –≤ —Å—Ç–æ—Ä–æ–Ω—É?
    
    –ï—Å–ª–∏ –ø–ª–æ—Ö–æ–π ‚Äî –ù–ï –°–û–•–†–ê–ù–Ø–¢–¨ –ö–ê–ö –ò–°–¢–ò–ù–£
    """
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ
    response_lower = response.lower()
    for prev in previous_responses[-3:]:  # last 3
        prev_lower = prev.lower()
        # –ï—Å–ª–∏ –±–æ–ª–µ–µ 60% —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ - —ç—Ç–æ –ø–æ–≤—Ç–æ—Ä
        overlap = len(set(response_lower.split()) & set(prev_lower.split()))
        total = len(set(response_lower.split()))
        if total > 0 and overlap / total > 0.6:
            logger.warning("‚úó Response rejected: too similar to previous")
            return False
    
    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã
    if len(response) < 10:
        logger.warning("‚úó Response rejected: too short")
        return False
    
    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ off-topic (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    forbidden = state.get("forbidden_topics", [])
    if forbidden:
        for topic in forbidden:
            if topic.lower() in response_lower:
                logger.warning(f"‚úó Response rejected: contains forbidden topic '{topic}'")
                return False
    
    return True

##################### üéØ MAIN CHAT PROCESSOR ####################

async def process_chat_message(user_id: str, session_id: str, user_message: str) -> ChatResponse:
    """
    üéØ –ì–õ–ê–í–ù–´–ô –ü–†–û–¶–ï–°–°–û–† –°–û–û–ë–©–ï–ù–ò–ô
    
    –®–∞–≥–∏:
    1. –ü–æ–ª—É—á–∏—Ç—å Profile + State + Summary + History
    2. –ò–∑–≤–ª–µ—á—å Intent –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è
    3. –û–±–Ω–æ–≤–∏—Ç—å Dialog State
    4. –°–æ–±—Ä–∞—Ç—å —É–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    5. –ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç LLM
    6. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –æ—Ç–≤–µ—Ç
    7. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–ª–∏ –æ—Ç–∫–ª–æ–Ω–∏—Ç—å
    """
    
    # Rate limit
    if not check_rate_limit(user_id, session_id):
        raise HTTPException(
            status_code=429,
            detail=f"Rate limit: max {RATE_LIMIT_REQUESTS} req per {RATE_LIMIT_WINDOW}s"
        )
    
    # Get session
    session = get_session(user_id, session_id)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")
    
    messages = session.get("messages", [])
    
    # STEP 1: Load memory tiers
    profile = get_user_profile(user_id)
    state = get_dialog_state(user_id, session_id)
    summary = get_summary(user_id, session_id)
    
    # STEP 2: Extract intent
    logger.info("üéØ Extracting intent...")
    new_state = await extract_intent(user_message, state, profile)
    
    # STEP 3: Update dialog state
    update_dialog_state(user_id, session_id, new_state)
    
    # STEP 4: Build smart prompt
    logger.info("üß† Building smart prompt...")
    system_content = build_system_prompt(profile, new_state, summary, messages)
    
    # Get rules for response metadata
    rules = build_response_rules(profile, new_state)
    
    # STEP 5: Get LLM response
    llm_messages = [
        {"role": "system", "content": system_content},
        {"role": "user", "content": user_message}
    ]
    
    result = await send_to_llm(llm_messages)
    ai_response = result["content"]
    
    # STEP 6: Validate response
    previous_responses = [m["content"] for m in messages if m.get("role") == "assistant"]
    
    if not validate_response(ai_response, new_state, previous_responses):
        # Retry once with stronger rules
        logger.warning("‚ö†Ô∏è  First response rejected, retrying with stricter rules...")
        rules.append("CRITICAL: This is a retry. Previous response was rejected for repetition or off-topic content.")
        rules.append("Provide a COMPLETELY DIFFERENT answer with NEW information.")
        
        system_content = build_system_prompt(profile, new_state, summary, messages)
        llm_messages[0]["content"] = system_content
        
        result = await send_to_llm(llm_messages, temperature=0.9)  # higher temp for variety
        ai_response = result["content"]
    
    # STEP 7: Save messages
    timestamp = datetime.utcnow().isoformat()
    messages.append({
        "role": "user",
        "content": user_message,
        "timestamp": timestamp
    })
    messages.append({
        "role": "assistant",
        "content": ai_response,
        "timestamp": timestamp
    })
    
    update_session(user_id, session_id, messages)
    
    # Update summary if needed
    new_summary = None
    if len(messages) >= SUMMARY_THRESHOLD:
        logger.info("üìù Updating summary...")
        new_summary = await generate_summary(summary, messages)
        update_summary(user_id, session_id, new_summary)
    
    return ChatResponse(
        user_id=user_id,
        session_id=session_id,
        response=ai_response,
        timestamp=timestamp,
        intent=new_state,
        summary=new_summary,
        tokens_used=result.get("tokens_used"),
        rules_applied=rules[:5]  # top 5 rules for debugging
    )

##################### FASTAPI APP ####################

app = FastAPI(
    title="Smart Chat Server v4.0",
    description="Intent-Driven Architecture with Dialog State",
    version="4.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

chat_router = APIRouter(prefix="/chat", tags=["chat"])
session_router = APIRouter(prefix="/session", tags=["session"])
profile_router = APIRouter(prefix="/profile", tags=["profile"])
summary_router = APIRouter(prefix="/summary", tags=["summary"])
state_router = APIRouter(prefix="/state", tags=["dialog_state"])

##################### HEALTH CHECK ####################

@app.get("/health")
async def health_check():
    try:
        r.ping()
        redis_status = "ok"
    except Exception as e:
        redis_status = f"error: {str(e)}"
    
    try:
        response = await asyncio.get_event_loop().run_in_executor(
            None,
            lambda: requests.get(f"http://{QWEN_HOST}:{QWEN_PORT}/health", timeout=2)
        )
        llm_status = "ok" if response.status_code == 200 else "error"
    except:
        llm_status = "unreachable"
    
    return {
        "status": "ok" if redis_status == "ok" and llm_status == "ok" else "degraded",
        "redis": redis_status,
        "llm": llm_status,
        "architecture": {
            "tier_1": "User Profile (passport - stable facts)",
            "tier_2": "Dialog State (intent - current goal)",
            "tier_3": "Summary (where we are now)",
            "tier_4": "History (raw messages for analysis)"
        },
        "features": {
            "intent_extraction": "automatic",
            "response_validation": "enabled",
            "dynamic_rules": "enabled",
            "modes": RESPONSE_MODES,
            "levels": USER_LEVELS
        },
        "timestamp": datetime.utcnow().isoformat()
    }

##################### CHAT ENDPOINTS ####################

@chat_router.post("/", response_model=ChatResponse)
async def chat(req: ChatRequest):
    """
    üéØ Main chat endpoint with intent-driven processing
    
    Features:
    - Automatic intent extraction
    - Dynamic response rules
    - Response validation
    - Smart context assembly
    """
    try:
        return await process_chat_message(req.user_id, req.session_id, req.message)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚úó Chat error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

##################### SESSION ENDPOINTS ####################

@session_router.post("/create")
def create_new_session(req: SessionCreate):
    """Create new session"""
    session_id = create_session(req.user_id, req.metadata)
    return {
        "user_id": req.user_id,
        "session_id": session_id,
        "created_at": datetime.utcnow().isoformat()
    }

@session_router.get("/{user_id}/{session_id}")
def get_session_info(user_id: str, session_id: str):
    """Get full session info with all memory tiers"""
    session = get_session(user_id, session_id)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")
    
    # Include all memory tiers
    profile = get_user_profile(user_id)
    state = get_dialog_state(user_id, session_id)
    summary = get_summary(user_id, session_id)
    
    return {
        **session,
        "profile": profile,
        "dialog_state": state,
        "summary": summary
    }

@session_router.delete("/{user_id}/{session_id}")
def remove_session(user_id: str, session_id: str):
    """Delete session and all associated data"""
    if not delete_session(user_id, session_id):
        raise HTTPException(status_code=404, detail="Session not found")
    return {"message": "Session deleted", "user_id": user_id, "session_id": session_id}

##################### PROFILE ENDPOINTS ####################

@profile_router.get("/{user_id}")
def read_user_profile(user_id: str):
    """Get user profile (passport)"""
    profile = get_user_profile(user_id)
    return {
        "user_id": user_id,
        "profile": profile
    }

@profile_router.post("/update")
def edit_user_profile(req: ProfileUpdate):
    """Update user profile"""
    current = get_user_profile(req.user_id)
    merged = merge_profile_facts(current, req.profile_data)
    update_user_profile(req.user_id, merged)
    return {
        "user_id": req.user_id,
        "profile": merged,
        "updated_at": datetime.utcnow().isoformat()
    }

@profile_router.delete("/{user_id}")
def delete_user_profile(user_id: str):
    """Delete user profile"""
    key = f"{USER_PROFILE_PREFIX}{user_id}"
    deleted = r.delete(key)
    if not deleted:
        raise HTTPException(status_code=404, detail="Profile not found")
    return {"message": "Profile deleted", "user_id": user_id}

##################### DIALOG STATE ENDPOINTS ####################

@state_router.get("/{user_id}/{session_id}")
def read_dialog_state(user_id: str, session_id: str):
    """Get current dialog state (intent)"""
    state = get_dialog_state(user_id, session_id)
    return {
        "user_id": user_id,
        "session_id": session_id,
        "dialog_state": state
    }

@state_router.post("/{user_id}/{session_id}/update")
def manual_state_update(user_id: str, session_id: str, state: Dict[str, Any]):
    """Manually update dialog state"""
    update_dialog_state(user_id, session_id, state)
    return {
        "user_id": user_id,
        "session_id": session_id,
        "dialog_state": state,
        "updated_at": datetime.utcnow().isoformat()
    }

@state_router.post("/{user_id}/{session_id}/reset")
def reset_dialog_state(user_id: str, session_id: str):
    """Reset dialog state to default"""
    default_state = {
        "current_goal": None,
        "mode": "learn",
        "detail_level": "normal",
        "understood_concepts": [],
        "forbidden_topics": [],
        "context_type": None
    }
    update_dialog_state(user_id, session_id, default_state)
    return {
        "message": "Dialog state reset",
        "user_id": user_id,
        "session_id": session_id,
        "dialog_state": default_state
    }

##################### SUMMARY ENDPOINTS ####################

@summary_router.get("/{user_id}/{session_id}")
def read_summary(user_id: str, session_id: str):
    """Get conversation summary"""
    summary = get_summary(user_id, session_id)
    return {
        "user_id": user_id,
        "session_id": session_id,
        "summary": summary if summary else "No summary available"
    }

@summary_router.post("/{user_id}/{session_id}/regenerate")
async def regenerate_summary(user_id: str, session_id: str):
    """Regenerate summary from current messages"""
    session = get_session(user_id, session_id)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")
    
    messages = session.get("messages", [])
    if not messages:
        raise HTTPException(status_code=400, detail="No messages to summarize")
    
    old_summary = get_summary(user_id, session_id)
    new_summary = await generate_summary(old_summary, messages)
    update_summary(user_id, session_id, new_summary)
    
    return {
        "user_id": user_id,
        "session_id": session_id,
        "summary": new_summary,
        "updated_at": datetime.utcnow().isoformat()
    }

@summary_router.delete("/{user_id}/{session_id}")
def remove_summary(user_id: str, session_id: str):
    """Delete summary"""
    key = f"{SUMMARY_PREFIX}{user_id}:{session_id}"
    deleted = r.delete(key)
    if not deleted:
        raise HTTPException(status_code=404, detail="Summary not found")
    return {"message": "Summary deleted", "user_id": user_id, "session_id": session_id}

##################### ANALYTICS & DEBUG ####################

@app.get("/stats")
def get_stats():
    """Server statistics and configuration"""
    return {
        "active_rate_limits": len(rate_limit_tracker),
        "llm_queue": LLM_CONCURRENCY - llm_semaphore._value,
        "config": {
            "max_tokens": MAX_RESPONSE_TOKENS,
            "max_history": MAX_HISTORY_LENGTH,
            "context_window": CONTEXT_WINDOW,
            "concurrency": LLM_CONCURRENCY,
            "rate_limit": f"{RATE_LIMIT_REQUESTS}/{RATE_LIMIT_WINDOW}s",
            "profile_update_threshold": PROFILE_UPDATE_THRESHOLD
        },
        "architecture": {
            "memory_tiers": 4,
            "intent_driven": True,
            "response_validation": True,
            "dynamic_rules": True
        },
        "supported": {
            "modes": RESPONSE_MODES,
            "levels": USER_LEVELS
        }
    }

@app.get("/debug/{user_id}/{session_id}")
async def debug_session(user_id: str, session_id: str):
    """
    üîç Debug endpoint - shows complete memory state
    
    Useful for understanding how the system works
    """
    session = get_session(user_id, session_id)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")
    
    profile = get_user_profile(user_id)
    state = get_dialog_state(user_id, session_id)
    summary = get_summary(user_id, session_id)
    rules = build_response_rules(profile, state)
    
    messages = session.get("messages", [])
    recent_messages = messages[-CONTEXT_WINDOW:] if messages else []
    
    return {
        "user_id": user_id,
        "session_id": session_id,
        "memory_tiers": {
            "tier_1_profile": profile,
            "tier_2_dialog_state": state,
            "tier_3_summary": summary,
            "tier_4_history_count": len(messages)
        },
        "context_sent_to_llm": {
            "profile_fields": [k for k, v in profile.items() if v],
            "dialog_state": state,
            "summary_length": len(summary) if summary else 0,
            "recent_messages_count": len(recent_messages),
            "recent_messages": recent_messages
        },
        "active_rules": rules,
        "system_prompt_preview": build_system_prompt(profile, state, summary, messages)[:500] + "..."
    }

##################### TESTING ENDPOINTS ####################

@app.post("/test/scenario")
async def test_scenario(user_id: str, scenario: str):
    """
    üß™ Test different user scenarios
    
    Scenarios:
    - beginner_learning
    - senior_debugging
    - quick_answers
    - detailed_explanation
    """
    
    # Create test session
    session_id = create_session(user_id, {"test_scenario": scenario})
    
    # Set profile based on scenario
    if scenario == "beginner_learning":
        profile = {
            "name": "TestUser",
            "level": "beginner",
            "role": "student",
            "tech_stack": [],
            "language": "en"
        }
        test_message = "How do I create a function in Python?"
        
    elif scenario == "senior_debugging":
        profile = {
            "name": "TestUser",
            "level": "senior",
            "role": "engineer",
            "tech_stack": ["Python", "FastAPI", "Redis"],
            "language": "en"
        }
        test_message = "My Redis connection keeps timing out in production"
        
    elif scenario == "quick_answers":
        profile = {
            "name": "TestUser",
            "level": "middle",
            "role": "developer",
            "tech_stack": ["JavaScript"],
            "language": "en"
        }
        test_message = "Quick: what's the difference between let and const?"
        
    elif scenario == "detailed_explanation":
        profile = {
            "name": "TestUser",
            "level": "junior",
            "role": "student",
            "tech_stack": ["React"],
            "language": "en"
        }
        test_message = "Explain React hooks in detail with examples"
        
    else:
        raise HTTPException(status_code=400, detail=f"Unknown scenario: {scenario}")
    
    # Update profile
    update_user_profile(user_id, profile)
    
    # Process message
    response = await process_chat_message(user_id, session_id, test_message)
    
    return {
        "scenario": scenario,
        "session_id": session_id,
        "test_message": test_message,
        "profile_used": profile,
        "response": response
    }

##################### INCLUDE ROUTERS ####################

app.include_router(chat_router)
app.include_router(session_router)
app.include_router(profile_router)
app.include_router(state_router)
app.include_router(summary_router)

##################### STARTUP/SHUTDOWN ####################

@app.on_event("startup")
async def startup_event():
    logger.info("=" * 80)
    logger.info("üöÄ SMART CHAT SERVER v4.0 - INTENT-DRIVEN ARCHITECTURE")
    logger.info("=" * 80)
    
    try:
        r.ping()
        logger.info("‚úì Redis: CONNECTED")
    except Exception as e:
        logger.error(f"‚úó Redis: FAILED - {e}")
    
    logger.info("")
    logger.info("üìö 4-TIER MEMORY ARCHITECTURE:")
    logger.info("   1Ô∏è‚É£  USER PROFILE    ‚Üí Passport (stable facts)")
    logger.info("   2Ô∏è‚É£  DIALOG STATE    ‚Üí Intent (current goal, mode)")
    logger.info("   3Ô∏è‚É£  SUMMARY         ‚Üí Where we are now")
    logger.info("   4Ô∏è‚É£  HISTORY         ‚Üí Raw messages (analysis only)")
    logger.info("")
    logger.info("üéØ KEY FEATURES:")
    logger.info("   ‚úì Automatic intent extraction from each message")
    logger.info("   ‚úì Dynamic response rules based on context")
    logger.info("   ‚úì Response validation (anti-repetition)")
    logger.info("   ‚úì Smart prompt assembly (only relevant context)")
    logger.info("")
    logger.info("‚öôÔ∏è  CONFIGURATION:")
    logger.info(f"   ‚Ä¢ Max tokens: {MAX_RESPONSE_TOKENS}")
    logger.info(f"   ‚Ä¢ History stored: {MAX_HISTORY_LENGTH} messages")
    logger.info(f"   ‚Ä¢ Context sent to LLM: {CONTEXT_WINDOW} messages")
    logger.info(f"   ‚Ä¢ Concurrency: {LLM_CONCURRENCY} parallel requests")
    logger.info(f"   ‚Ä¢ Rate limit: {RATE_LIMIT_REQUESTS} req/{RATE_LIMIT_WINDOW}s")
    logger.info("")
    logger.info("üîß SUPPORTED MODES:")
    logger.info(f"   ‚Ä¢ Response modes: {', '.join(RESPONSE_MODES)}")
    logger.info(f"   ‚Ä¢ User levels: {', '.join(USER_LEVELS)}")
    logger.info("")
    logger.info("üß™ TEST ENDPOINTS:")
    logger.info("   ‚Ä¢ POST /test/scenario - Test different user scenarios")
    logger.info("   ‚Ä¢ GET /debug/{user_id}/{session_id} - Full memory inspection")
    logger.info("")
    logger.info("=" * 80)
    logger.info("LLM = –ò–°–ü–û–õ–ù–ò–¢–ï–õ–¨ | BACKEND = –ú–û–ó–ì")
    logger.info("=" * 80)

@app.on_event("shutdown")
async def shutdown_event():
    logger.info("üëã Shutting down Smart Chat Server v4.0...")

##################### MAIN ####################

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        workers=1,
        log_level="info"
    )